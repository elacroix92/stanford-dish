---
title: "General Soil Properties"
author: "Emily Lacroix"
date: "Last update: 7/1/2022"
output: 
  github_document:
    toc: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Setup

## Load libraries
```{r message=FALSE, warning=FALSE}
library(multcompView)
library(car)
library(tidyverse)
library(readxl)

options(dplyr.summarise.inform = FALSE)
```

## Files

```{r eval=FALSE}
data_file <- "StanfordDish_AllData.xlsx"
```

```{r include=FALSE}
data_file <- "~/Box Sync/Stanford (elacroix@stanford.edu)/Research/StanfordDish/Data/StanfordDish_AllData.xlsx"
```

## Constants 
For texture analysis, all solutions were at 23 degrees C. 
```{r}

concentration_hmp_g_mL <- 0.005
viscosity_hmp_g_cm_s <- 0.9533 #value from table in Gee and Bauder 1986
density_water_g_cm3 <- 1.0035 #specific to temperature 
particle_density_g_cm3 <- 2.65
g_cm_s2 <- 980.665
hmp_density <- density_water_g_cm3 * (1 + (0.63 * concentration_hmp_g_mL))
beta_calc <- 30 * viscosity_hmp_g_cm_s / (g_cm_s2 * (particle_density_g_cm3 - hmp_density))

```

## Useful Functions
```{r}
tri.to.squ<-function(x)
{
  rn <- row.names(x)
  cn <- colnames(x)
  an <- unique(c(cn,rn))
  myval <- x[!is.na(x)]
  mymat <- 
    matrix(1, nrow = length(an), ncol = length(an), dimnames = list(an,an))
  for(ext in 1:length(cn))
  {
    for(int in 1:length(rn))
    {
      if(is.na(x[row.names(x) == rn[int], colnames(x) == cn[ext]])) next
      mymat[row.names(mymat) == rn[int], colnames(mymat) == cn[ext]] <- 
        x[row.names(x) == rn[int], colnames(x) == cn[ext]]
      mymat[row.names(mymat) == cn[ext], colnames(mymat) == rn[int]] <- 
        x[row.names(x) == rn[int], colnames(x) == cn[ext]]
    }
  }
  return(mymat)
}
```

# Texture
This code calculates texture based on hydrometer readings.

## Import and wrangle data
```{r}
data <- 
  data_file %>% 
  read_xlsx(sheet = "Texture") %>% 
  mutate(
    p = 100* (hydrometer_reading_g_L - blank) / mass_g,
    h_prime = -0.164 * hydrometer_reading_g_L + 16.3,
    theta = 1000 * (beta_calc * h_prime)^0.5,
    x_um = theta * (time_min)^-0.5,
    ln_x = log(x_um) #log is the natural logarithm in R
  ) %>% 
  pivot_wider(
    id_cols = 
      c("site", "experiment", "core_num", "blank", "temperature_c", "mass_g"),
    names_from = time_min,
    values_from = c(p, h_prime, theta, x_um, ln_x)
  ) 

```

## Calculate texture

### Write function to calculate texture from hydrometer readings
```{r}
calculate_texture <-
  function(p_90, p_1440, ln_x_90, ln_x_1440, p_half, p_1, ln_x_half, ln_x_1){
    m_clay <- (p_90 - p_1440) / (ln_x_90 - ln_x_1440)
    p_2um <- m_clay*log(2) + p_1440
    
    m_sand <- (p_half - p_1) / (ln_x_half - ln_x_1)
    p_50um <- m_sand*log(50) + p_1440
    
    sand <- 100 - p_50um
    silt <- p_50um - p_2um
    clay <- p_2um
    
    print(paste(sand, silt, clay))
  }

```


### Calculate texture for each sample
```{r}
texture <- 
  data %>% 
  mutate(
    sand_silt_clay = 
      calculate_texture(
        p_90 = p_90, 
        p_1440 = p_1440, 
        ln_x_90 = ln_x_90, 
        ln_x_1440 = ln_x_1440, 
        p_half = p_0.5, 
        p_1 = p_1,
        ln_x_half = ln_x_0.5,
        ln_x_1 = ln_x_1
      )
  ) %>% 
  separate(sand_silt_clay, sep = " ", into = c("sand", "silt", "clay")) %>% 
  mutate(across(c(sand, silt, clay), as.numeric)) %>% 
  select(site, experiment, core_num, sand, silt, clay)


```
### Summarize texture data
```{r}
texture %>% 
  group_by(site) %>% 
  summarise(
    across(c(sand, silt, clay), list(mean = mean, se = ~sd(.)/sqrt(n()))),
    clay_normality_p = shapiro.test(clay)$p.value,
    silt_normality_p = shapiro.test(silt)$p.value,
    sand_normality_p = shapiro.test(sand)$p.value
  )

```
DISHC texture = clay loam
DISHR texture = sandy loam
DISHT texture = loam


### Test for differences amongst sites

Sand and clay are normally distributed. Silt is not. 
```{r}

leveneTest(clay ~ site, data = texture)

leveneTest(sand ~ site, data = texture)

leveneTest(silt ~ site, data = texture)

```

AOV for clay and sand
```{r}
sand_aov <- aov(sand ~ site, data = texture)
summary(sand_aov)

clay_aov <- aov(clay ~ site, data = texture)
summary(clay_aov)
TukeyHSD(clay_aov)

```


Pairwise Wilcoxon for silt
```{r}
silt_wilcox <- 
  pairwise.wilcox.test(
  x = texture$silt,
  g = texture$site,
  p.adjust.method = "BH",
  paired = FALSE
)

silt_wilcox

silt_matrix_site <- tri.to.squ(silt_wilcox$p.value)
silt_letters_site <- multcompLetters(silt_matrix_site, compare ="<=", threshold = 0.05, Letters = letters)

silt_letters_site
```



# Aggregate Size 

## Calculate log average aggregate size D

```{r}

max_size <- 50
sieve1 <- 27
sieve2 <- 19
sieve3 <- 9.4
sieve4 <- 6.7
sieve5 <- 4.7
sieve6 <- 2
pan <- 0

log_sizes <-
  tibble(
    name = 
      c(
        "size1",
        "size2", 
        "size3", 
        "size4", 
        "size5",
        "size6",
        "size7"
      ), 
    log_size = 
      c(
        log(mean(c(max_size, sieve1))),
        log(mean(c(sieve1, sieve2))),
        log(mean(c(sieve2, sieve3))),
        log(mean(c(sieve3, sieve4))),
        log(mean(c(sieve4, sieve5))),
        log(mean(c(sieve5, sieve6))),
        log(mean(c(sieve6, pan)))
        ),
    avg_size = 
      c(
        mean(c(max_size, sieve1)),
        mean(c(sieve1, sieve2)),
        mean(c(sieve2, sieve3)),
        mean(c(sieve3, sieve4)),
        mean(c(sieve4, sieve5)),
        mean(c(sieve5, sieve6)),
        mean(c(sieve6, pan))
        )
  )

sieve_key <- 
  c(
    `27mm` = "size1", 
    `19mm` = "size2", 
    `9.4mm` = "size3", 
    `6.7mm` = "size4", 
    `4.7mm` = "size5",
    `2mm` = "size6",
    `less_2_mm` = "size7"
  )

log_sizes


```


## Import data and calculate geometric mean diameter and meanweight diameter

```{r}
aggregate <-
  data_file %>% 
  read_xlsx(sheet = "AggregateSize") %>% 
  pivot_longer(cols = ends_with("mm"), names_to = "size", values_to = "mass_g") %>% 
  mutate(across(size, ~recode(., !!!sieve_key))) %>% 
  left_join(log_sizes, by = c("size" = "name")) %>% 
  mutate(
    wi_logx = mass_g * log_size,
    wi_x = (mass_g / total_mass_g) * avg_size
  ) %>% 
  group_by(sample, rep) %>% 
  summarise(
    sum_wi_logx = sum(wi_logx),
    sum_wi = sum(mass_g),
    mwd = sum(wi_x)
  ) %>% 
  mutate(
    gmd = exp(sum_wi_logx / sum_wi)
  )

aggregate
```

## Summarise aggregate size by site

```{r}

aggregate %>% 
  group_by(sample) %>% 
  summarise(
    mean_mwd = mean(mwd),
    se_mwd = sd(mwd) / sqrt(n()),
    mean_gmd = mean(gmd),
    se_gmd = sd(gmd) / sqrt(n()),
    normality_p_mwd = shapiro.test(mwd)$p.value,
    normality_p_gmd = shapiro.test(gmd)$p.value
  )

```

## Statistics - MWD & GMD

### MWD - ANOVA
```{r}

leveneTest(mwd ~ sample, data = aggregate)

aov_mwd <- aov(mwd ~ sample, data = aggregate)
summary(aov_mwd)
TukeyHSD(aov_mwd)

```

### GMD - Wilcoxon rank sum
```{r}

gmd_wilcox <- 
  pairwise.wilcox.test(
  x = aggregate$gmd,
  g = aggregate$sample,
  p.adjust.method = "BH",
  paired = FALSE
)

gmd_wilcox

gmd_matrix_site <- tri.to.squ(gmd_wilcox$p.value)
gmd_letters_site <- multcompLetters(gmd_matrix_site, compare ="<=", threshold = 0.05, Letters = letters)

gmd_letters_site


```



# Total Carbon and Nitrogen

## Import and summarise data 

```{r}
carbon_nitrogen <- 
  data_file %>% 
  read_xlsx(sheet = "TotalCarbon") %>% 
  group_by(site) %>% 
  mutate(
    n = n()
  ) %>% 
  group_by(site, n) %>% 
  summarise(across(c(perc_c, perc_n), list(mean = mean, sd = sd))) %>% 
  mutate(
    se_perc_c = perc_c_sd / sqrt(n),
    se_perc_n = perc_n_sd / sqrt(n),
  ) %>% 
  select(site, perc_c_mean, se_perc_c, perc_n_mean, se_perc_n)

```

```{r}
carbon_nitrogen %>% knitr::kable()
```


## Statistics - total C

### Check for normality

```{r}
 
data_file %>% 
  read_xlsx(sheet = "TotalCarbon") %>% 
  group_by(site) %>% 
  summarise(
    p_normality = shapiro.test(perc_c)$p.value
  )

```
Percent C is normally distributed

### Check for equal variances

```{r}

leveneTest(
  perc_c ~ site, 
  data = data_file %>% 
    read_xlsx(sheet = "TotalCarbon")
)

```

Variances are equal.

### ANOVA

```{r}
perc_c_aov <- 
  aov(
  perc_c ~ site, 
  data = data_file %>% 
    read_xlsx(sheet = "TotalCarbon")
)

summary(perc_c_aov)
```

Significant differences. Do TukeyHSD as post-hoc test.

```{r}
TukeyHSD(perc_c_aov)
```

## Statistics - C/N

```{r}

c_to_n <- 
  data_file %>% 
  read_xlsx(sheet = "TotalCarbon") %>% 
  mutate(c_to_n = perc_c / perc_n)

```

### Check normality 
```{r}
  
c_to_n %>% 
  group_by(site) %>% 
  summarise(
    avg_c_to_n = mean(c_to_n, na.rm = TRUE),
    se_c_to_n = sd(c_to_n) / sqrt(n()),
    p_normality = shapiro.test(c_to_n)$p.value,
    p_normality2 = shapiro.test(c_to_n^2)$p.value,
  )

```

```{r}

c_to_n %>% 
  ggplot(aes(x = c_to_n^2)) + 
  geom_histogram() + 
  facet_grid(cols = vars(site))
```

Will try a squared transformation 


### Check for equal variances  
```{r}
leveneTest(c_to_n^2 ~ site, data = c_to_n)
```


With a transformation, the C to N ratio can be normally distributed but no equal variances. 
Will need to use pairwise Wilcoxon rank sum 

### Wilcoxon rank sum 

```{r}
c_to_n_wilcox <- 
  pairwise.wilcox.test(
  x = c_to_n$c_to_n,
  g = c_to_n$site,
  p.adjust.method = "BH",
  paired = FALSE
)

c_to_n_wilcox

c_to_n_matrix_site <- tri.to.squ(c_to_n_wilcox$p.value)
c_to_n_letters_site <- multcompLetters(c_to_n_matrix_site, compare ="<=", threshold = 0.05, Letters = letters)

c_to_n_letters_site
```

